setwd("C:/Users/alexg/Google Drive/05-Proyectos/restlab_workshop")
setwd("C:/Users/alexg/Google Drive/05-Proyectos/restlab_workshop")
rm(list=ls())
options(stringsAsFactors = F)
#total wells
tw<-96
#filename
fn<-"LAA-E2.xlsx"
#replicates?
x<-3
#number of antibiotic concentrations
nc<-12
#number of strains #JUST AURLI
ns<-1
library("readxl")
allSheets<-list()
for (i in 1:c((length(excel_sheets(fn))))){   #This will read in each sheet of excel file 'fn', if there is a blank sheet at the end of excel file this line will be "for (i in 1:c((length(excel_sheets(fn)))-1)){"
mySheet <- read_excel(fn, sheet = i)  #This will import a sheet as a dataframe
write.csv(mySheet, file = paste("t", i,".csv", sep = ""), row.names = FALSE)   #This will save each sheet as a csv with the file name 't.x', x = sheet number
mySheet<-read.csv(paste("t", paste(i),".csv", sep= ""), header = T, skip = 25, stringsAsFactors=FALSE) #This will read in the data starting at the line number after 'skip='
mySheet<-mySheet[,2:13] #This will filter out your data [row,column], removing row names
allSheets[[i]]<-mySheet[1:3,] #preserved only first three rows (aurli) and stored data as a list element in allSheets
}
options(stringsAsFactors = F)
#total wells
tw<-96
#filename
fn<-"LAA-E2//LAA-E2.xlsx"
#replicates?
x<-3
#number of antibiotic concentrations
nc<-12
#number of strains #JUST AURLI
ns<-1
library("readxl")
allSheets<-list()
for (i in 1:c((length(excel_sheets(fn))))){   #This will read in each sheet of excel file 'fn', if there is a blank sheet at the end of excel file this line will be "for (i in 1:c((length(excel_sheets(fn)))-1)){"
mySheet <- read_excel(fn, sheet = i)  #This will import a sheet as a dataframe
write.csv(mySheet, file = paste("t", i,".csv", sep = ""), row.names = FALSE)   #This will save each sheet as a csv with the file name 't.x', x = sheet number
mySheet<-read.csv(paste("t", paste(i),".csv", sep= ""), header = T, skip = 25, stringsAsFactors=FALSE) #This will read in the data starting at the line number after 'skip='
mySheet<-mySheet[,2:13] #This will filter out your data [row,column], removing row names
allSheets[[i]]<-mySheet[1:3,] #preserved only first three rows (aurli) and stored data as a list element in allSheets
}
head(allSheets)
#reverse list order for excel files in reverse chronological order
library("rlist")
allSheets1<-list.reverse(allSheets)
#calculate difference in time
sto1.t<-list()
for (i in 1:c(length(excel_sheets(fn)))) {
sto.t<-read.csv(paste("t", paste(i),".csv", sep= ""), header = F, skip = 22, nrow = 1, stringsAsFactors=FALSE)
#date & time
sto.t<-sto.t[1,2]
#strsplit date & replace "/" with "-"
date<-gsub("/", "-", paste((strsplit(sto.t, " ")[[1]][(1)]), collapse = " "))
#date format from m-d-y to y-m-d
y<-(strsplit(date, "-")[[1]][(3)])
m<-(strsplit(date, "-")[[1]][(1)])
d<-(strsplit(date, "-")[[1]][(2)])
tdate<-paste(y, m,d, sep = "-")
#put into list
sto1.t[[i]]<-tdate
#strsplit time
t<-paste((strsplit(sto.t, " ")[[1]][(2:3)]), collapse = " ")
#change 12H to 24H
ttime<-format(strptime(t, "%I:%M:%S %p"), format="%H:%M:%S")
sto1.t[[i]][2]<-ttime
}
sto1.t<-list.reverse(sto1.t)
tdif<-c()
for (i in 2:length(sto1.t)) {
dif<-difftime(paste(sto1.t[[i]][1],sto1.t[[i]][2], sep = " "), paste(sto1.t[[1]][1],sto1.t[[1]][2], sep = " "), units = "hours")
tdif<-c(tdif, dif)
}
tdif<-c(0,tdif)
head(tdif)
#create the data.frame structure and column names
u<-data.frame(time=c(tdif))
ro<-c("A","B","C")
co<-c(1:12)
df<-c() #makes a vector of all the well names used in 'ro' and 'co'
for (i in (unique(ro))) {
for (j in (unique(co))) {
df<-c(df,paste(i,j,sep=""))
}
}
for (i in 1:length(df)) { #creates each well name to be a vector in existence
assign(df[i], c(recursive = TRUE))
}
for (i in 1:length(df)) { #assigns vector of well name and column of u with data of each time point at well placement
for (j in 1:length(allSheets1)) {
assign(df[i], c(get(df[i]),as.numeric(c(allSheets1[[j]][1,],allSheets1[[j]][2,],allSheets1[[j]][3,]))[i])) # makes a vector of data to sleect based on df[i] // optimization required here for more than three rows
}
u[,(i+1)]<-get(df[i])
}
colnames(u)<-c("time",df) #give each column the corresponding well name
head(u)
library("growthcurver")
gc_out <- SummarizeGrowthByPlate(u)
gc_out$note?  unique(gc_out$note)
d<-u #renames our data to match growthcurver's provided script
num_analyses <- length(names(d)) - 1
d_gc <- data.frame(sample = character(num_analyses),
k = numeric(num_analyses),
n0  = numeric(num_analyses),
r = numeric(num_analyses),
t_mid = numeric(num_analyses),
t_gen = numeric(num_analyses),
auc_l = numeric(num_analyses),
auc_e = numeric(num_analyses),
sigma = numeric(num_analyses),
stringsAsFactors = FALSE)
trim_at_time<-tdif[8] #set our preferred plotting range based on tdif
#pdf("LAA-E2_growthcurver_r.pdf", height = 8.5, width = 11) #to print a pdf remove the hashtag at this line AND at line below = 'dev.off()'
par(mfrow = c(8,12))
par(mar = c(0.25,0.25,0.25,0.25))
y_lim_max <- max(d[,setdiff(names(d), "time")]) - min(d[,setdiff(names(d), "time")])
n <- 1    # keeps track of the current row in the output data frame
for (col_name in names(d)) {
# Don't process the column called "time".
# It contains time and not absorbance data.
if (col_name != "time") {
# Create a temporary data frame that contains just the time and current col
d_loop <- d[, c("time", col_name)]
# Do the background correction.
# Background correction option 1: subtract the minimum value in a column
#                                 from all measurements in that column
min_value <- min(d_loop[, col_name])
d_loop[, col_name] <- d_loop[, col_name] - min_value
# Background correction option 2: subtract the mean value of blank wells
#                                 over the course the experiment
#                                 (Replace B2, D8, G11 with the column
#                                  names of your media-only wells)
#d$blank <- apply(d[, c("B2", "D8", "G11")], 1, mean)
#d$A1 <- d$A1 - d$blank
# Now, call Growthcurver to calculate the metrics using SummarizeGrowth
gc_fit <- SummarizeGrowth(data_t = d_loop[, "time"],
data_n = d_loop[, col_name],
t_trim = trim_at_time,
bg_correct = "none")
# Now, add the metrics from this column to the next row (n) in the
# output data frame, and increment the row counter (n)
d_gc$sample[n] <- col_name
d_gc[n, 2:9] <- c(gc_fit$vals$k,
gc_fit$vals$n0,
gc_fit$vals$r,
gc_fit$vals$t_mid,
gc_fit$vals$t_gen,
gc_fit$vals$auc_l,
gc_fit$vals$auc_e,
gc_fit$vals$sigma)
n <- n + 1
# Finally, plot the raw data and the fitted curve
# Here, I'll just print some of the data points to keep the file size smaller
n_obs <- length(gc_fit$data$t)
idx_to_plot <- 1:20 / 20 * n_obs
plot(gc_fit$data$t[idx_to_plot], gc_fit$data$N[idx_to_plot],
pch = 20,
xlim = c(0, trim_at_time),
ylim = c(0, y_lim_max),
cex = 0.6, xaxt = "n", yaxt = "n")
text(x = trim_at_time / 4, y = y_lim_max, labels = col_name, pos = 1)
if (gc_fit$model=="") {
gc_fit$model<-rep(0,length(tdif[1:grep(trim_at_time, tdif)])-1)
lines(gc_fit$data$t, rep(0,length(tdif[1:grep(trim_at_time, tdif)])-1), col = "indianred3")
}
else #I added 'if...else' for data where no model was fit plots line at y=0
lines(gc_fit$data$t, predict(gc_fit$model), col = "indianred3")
}
}
#dev.off() #to print a pdf remove the hashtag at this line AND at 'pdf(...)' line above
# Load dplyr, ggplot2, and the sample data
library(dplyr)
library(ggplot2)
pca_gc_out <- as_data_frame(gc_out)
# Prepare the gc_out data for the PCA
rownames(pca_gc_out) <- pca_gc_out$sample
# Do the PCA
pca.res <- prcomp(pca_gc_out %>% select(k:sigma), center=TRUE, scale=TRUE)
# Plot the results
as_data_frame(list(PC1=pca.res$x[,1],
PC2=pca.res$x[,2],
samples = rownames(pca.res$x))) %>%
ggplot(aes(x=PC1,y=PC2, label=samples)) +
geom_text(size = 3)
# Do the PCA with percentages in axes
pca.res <- prcomp(pca_gc_out %>% select(k:sigma), center=TRUE, scale=TRUE)
percentage <- round(pca.res$sdev / sum(pca.res$sdev) * 100, 2)
df_out <- as.data.frame(pca.res$x)
percentage <- paste( colnames(df_out), "(", paste( as.character(percentage), "%", ")", sep="") )
# Plot the results
as_data_frame(list(PC1=pca.res$x[,1],
PC2=pca.res$x[,2],
samples = rownames(pca.res$x))) %>%
ggplot(aes(x=PC1,y=PC2, label=samples)) + geom_text(size = 3) + xlab(percentage[1]) + ylab(percentage[2])
par(mfrow=c(1,1),mar = c(3.8,3.8,2,2))
# Plot a histogram of the sigma values in order to check for outliers
hist(gc_out$sigma, breaks= 30,main = "Histogram of sigma values", xlab = "sigma", col = "palegreen3", xlim = c(0, max(gc_out$sigma)))
par(mfrow = c(8,12))
par(mar = c(0.25,0.25,0.25,0.25))
y<-c(min(gc_out$k), max(gc_out$k))
for (i in 1:12) {
boxplot(gc_out$k[c(i,i+12, i+24)], col="maroon", ylim=y, yaxt="n")
}
y<-c(min(gc_out$r), max(gc_out$r))
for (i in 1:12) {
boxplot(gc_out$r[c(i,i+12, i+24)], col="light coral", ylim=y,yaxt="n")
}
y<-c(min(gc_out$t_mid), max(gc_out$t_mid))
for (i in 1:12) {
boxplot(gc_out$t_mid[c(i,i+12, i+24)], col="goldenrod3", ylim=y,yaxt="n")
}
y<-c(min(gc_out$t_gen), max(gc_out$t_gen))
for (i in 1:12) {
boxplot(gc_out$t_gen[c(i,i+12, i+24)], col="midnightblue", ylim=y,yaxt="n")
}
y<-c(min(gc_out$auc_l), max(gc_out$auc_l))
for (i in 1:12) {
boxplot(gc_out$auc_l[c(i,i+12, i+24)], col="mediumpurple3", ylim=y,yaxt="n")
}
y<-c(min(gc_out$auc_e), max(gc_out$auc_e))
for (i in 1:12) {
boxplot(gc_out$auc_e[c(i,i+12, i+24)], col="tomato3", ylim=y,yaxt="n")
}
options(stringsAsFactors = F)
setwd("C:/Users/maria/Downloads/NGE-E1")
setwd("C:/Users/alexg/Google Drive/05-Proyectos/restlab_workshop/MarianaRius")
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_knit$set(root.dir = "C:/Users/alexg/Google Drive/05-Proyectos/restlab_workshop/MarianaRius")
install.packages("readxl")
install.packages("rlist")
install.packages("growthcurver")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("readxl")
install.packages("rlist")
install.packages("growthcurver")
install.packages("dplyr")
knitr::opts_knit$set(root.dir = "C:/Users/alexg/Google Drive/05-Proyectos/restlab_workshop/MarianaRius")
options(stringsAsFactors = F)
#total wells
tw<-96
#filename
fn<-"LAA-E2//LAA-E2.xlsx"
#replicates?
x<-3
#number of antibiotic concentrations
nc<-12
#number of strains #JUST AURLI
ns<-1
library("readxl")
allSheets<-list()
for (i in 1:c((length(excel_sheets(fn))))){   #This will read in each sheet of excel file 'fn', if there is a blank sheet at the end of excel file this line will be "for (i in 1:c((length(excel_sheets(fn)))-1)){"
mySheet <- read_excel(fn, sheet = i)  #This will import a sheet as a dataframe
write.csv(mySheet, file = paste("t", i,".csv", sep = ""), row.names = FALSE)   #This will save each sheet as a csv with the file name 't.x', x = sheet number
mySheet<-read.csv(paste("t", paste(i),".csv", sep= ""), header = T, skip = 25, stringsAsFactors=FALSE) #This will read in the data starting at the line number after 'skip='
mySheet<-mySheet[,2:13] #This will filter out your data [row,column], removing row names
allSheets[[i]]<-mySheet[1:3,] #preserved only first three rows (aurli) and stored data as a list element in allSheets
}
library("readxl")
allSheets<-list()
for (i in 1:c((length(excel_sheets(fn))))){   #This will read in each sheet of excel file 'fn', if there is a blank sheet at the end of excel file this line will be "for (i in 1:c((length(excel_sheets(fn)))-1)){"
mySheet <- read_excel(fn, sheet = i)  #This will import a sheet as a dataframe
write.csv(mySheet, file = paste("t", i,".csv", sep = ""), row.names = FALSE)   #This will save each sheet as a csv with the file name 't.x', x = sheet number
mySheet<-read.csv(paste("t", paste(i),".csv", sep= ""), header = T, skip = 25, stringsAsFactors=FALSE) #This will read in the data starting at the line number after 'skip='
mySheet<-mySheet[,2:13] #This will filter out your data [row,column], removing row names
allSheets[[i]]<-mySheet[1:3,] #preserved only first three rows (aurli) and stored data as a list element in allSheets
}
options(stringsAsFactors = F)
#total wells
tw<-96
#filename
fn<-"data/LAA-E2//LAA-E2.xlsx"
#replicates?
x<-3
#number of antibiotic concentrations
nc<-12
#number of strains #JUST AURLI
ns<-1
library("readxl")
allSheets<-list()
for (i in 1:c((length(excel_sheets(fn))))){   #This will read in each sheet of excel file 'fn', if there is a blank sheet at the end of excel file this line will be "for (i in 1:c((length(excel_sheets(fn)))-1)){"
mySheet <- read_excel(fn, sheet = i)  #This will import a sheet as a dataframe
write.csv(mySheet, file = paste("t", i,".csv", sep = ""), row.names = FALSE)   #This will save each sheet as a csv with the file name 't.x', x = sheet number
mySheet<-read.csv(paste("t", paste(i),".csv", sep= ""), header = T, skip = 25, stringsAsFactors=FALSE) #This will read in the data starting at the line number after 'skip='
mySheet<-mySheet[,2:13] #This will filter out your data [row,column], removing row names
allSheets[[i]]<-mySheet[1:3,] #preserved only first three rows (aurli) and stored data as a list element in allSheets
}
getwd()
knitr::opts_knit$set(root.dir = "C:/Users/alexg/Google Drive/05-Proyectos/restlab_workshop/MarianaRius")
getwd()
knitr::opts_knit$set(root.dir = "C:/Users/alexg/Google Drive/05-Proyectos/restlab_workshop/MarianaRius")
getwd()
knitr::opts_knit$set(root.dir = "C:/Users/alexg/Google Drive/05-Proyectos/restlab_workshop/MarianaRius")
options(stringsAsFactors = F)
#total wells
tw<-96
#filename
fn<-"../data/LAA-E2//LAA-E2.xlsx"
#replicates?
x<-3
#number of antibiotic concentrations
nc<-12
#number of strains #JUST AURLI
ns<-1
knitr::opts_knit$set(root.dir = "C:/Users/alexg/Google Drive/05-Proyectos/restlab_workshop/MarianaRius")
getwd()
knitr::opts_knit$set(root.dir = "C:/Users/alexg/Google Drive/05-Proyectos/restlab_workshop/MarianaRius")
getwd()
opts_chunk$set(root.dir = 'C:/Users/alexg/Google Drive/05-Proyectos/restlab_workshop/MarianaRius')
knitr::opts_knit$set(root.dir = 'C:/Users/alexg/Google Drive/05-Proyectos/restlab_workshop/MarianaRius')
getwd()
knitr::opts_knit$set(root.dir = 'C:/Users/alexg/Google Drive/05-Proyectos/restlab_workshop/MarianaRius')
options(stringsAsFactors = F)
#total wells
tw<-96
#filename
fn<-"data/LAA-E2//LAA-E2.xlsx"
#replicates?
x<-3
#number of antibiotic concentrations
nc<-12
#number of strains #JUST AURLI
ns<-1
library("readxl")
allSheets<-list()
for (i in 1:c((length(excel_sheets(fn))))){   #This will read in each sheet of excel file 'fn', if there is a blank sheet at the end of excel file this line will be "for (i in 1:c((length(excel_sheets(fn)))-1)){"
mySheet <- read_excel(fn, sheet = i)  #This will import a sheet as a dataframe
write.csv(mySheet, file = paste("t", i,".csv", sep = ""), row.names = FALSE)   #This will save each sheet as a csv with the file name 't.x', x = sheet number
mySheet<-read.csv(paste("t", paste(i),".csv", sep= ""), header = T, skip = 25, stringsAsFactors=FALSE) #This will read in the data starting at the line number after 'skip='
mySheet<-mySheet[,2:13] #This will filter out your data [row,column], removing row names
allSheets[[i]]<-mySheet[1:3,] #preserved only first three rows (aurli) and stored data as a list element in allSheets
}
head(allSheets)
#reverse list order for excel files in reverse chronological order
library("rlist")
allSheets1<-list.reverse(allSheets)
#calculate difference in time
sto1.t<-list()
for (i in 1:c(length(excel_sheets(fn)))) {
sto.t<-read.csv(paste("t", paste(i),".csv", sep= ""), header = F, skip = 22, nrow = 1, stringsAsFactors=FALSE)
#date & time
sto.t<-sto.t[1,2]
#strsplit date & replace "/" with "-"
date<-gsub("/", "-", paste((strsplit(sto.t, " ")[[1]][(1)]), collapse = " "))
#date format from m-d-y to y-m-d
y<-(strsplit(date, "-")[[1]][(3)])
m<-(strsplit(date, "-")[[1]][(1)])
d<-(strsplit(date, "-")[[1]][(2)])
tdate<-paste(y, m,d, sep = "-")
#put into list
sto1.t[[i]]<-tdate
#strsplit time
t<-paste((strsplit(sto.t, " ")[[1]][(2:3)]), collapse = " ")
#change 12H to 24H
ttime<-format(strptime(t, "%I:%M:%S %p"), format="%H:%M:%S")
sto1.t[[i]][2]<-ttime
}
sto1.t<-list.reverse(sto1.t)
tdif<-c()
for (i in 2:length(sto1.t)) {
dif<-difftime(paste(sto1.t[[i]][1],sto1.t[[i]][2], sep = " "), paste(sto1.t[[1]][1],sto1.t[[1]][2], sep = " "), units = "hours")
tdif<-c(tdif, dif)
}
tdif<-c(0,tdif)
#create the data.frame structure and column names
u<-data.frame(time=c(tdif))
ro<-c("A","B","C")
co<-c(1:12)
df<-c() #makes a vector of all the well names used in 'ro' and 'co'
for (i in (unique(ro))) {
for (j in (unique(co))) {
df<-c(df,paste(i,j,sep=""))
}
}
for (i in 1:length(df)) { #creates each well name to be a vector in existence
assign(df[i], c(recursive = TRUE))
}
for (i in 1:length(df)) { #assigns vector of well name and column of u with data of each time point at well placement
for (j in 1:length(allSheets1)) {
assign(df[i], c(get(df[i]),as.numeric(c(allSheets1[[j]][1,],allSheets1[[j]][2,],allSheets1[[j]][3,]))[i])) # makes a vector of data to sleect based on df[i] // optimization required here for more than three rows
}
u[,(i+1)]<-get(df[i])
}
colnames(u)<-c("time",df) #give each column the corresponding well name
library("growthcurver")
gc_out <- SummarizeGrowthByPlate(u)
gc_out$note?  unique(gc_out$note)
d<-u #renames our data to match growthcurver's provided script
num_analyses <- length(names(d)) - 1
d_gc <- data.frame(sample = character(num_analyses),
k = numeric(num_analyses),
n0  = numeric(num_analyses),
r = numeric(num_analyses),
t_mid = numeric(num_analyses),
t_gen = numeric(num_analyses),
auc_l = numeric(num_analyses),
auc_e = numeric(num_analyses),
sigma = numeric(num_analyses),
stringsAsFactors = FALSE)
trim_at_time<-tdif[8] #set our preferred plotting range based on tdif
#pdf("LAA-E2_growthcurver_r.pdf", height = 8.5, width = 11) #to print a pdf remove the hashtag at this line AND at line below = 'dev.off()'
par(mfrow = c(8,12))
par(mar = c(0.25,0.25,0.25,0.25))
y_lim_max <- max(d[,setdiff(names(d), "time")]) - min(d[,setdiff(names(d), "time")])
n <- 1    # keeps track of the current row in the output data frame
for (col_name in names(d)) {
# Don't process the column called "time".
# It contains time and not absorbance data.
if (col_name != "time") {
# Create a temporary data frame that contains just the time and current col
d_loop <- d[, c("time", col_name)]
# Do the background correction.
# Background correction option 1: subtract the minimum value in a column
#                                 from all measurements in that column
min_value <- min(d_loop[, col_name])
d_loop[, col_name] <- d_loop[, col_name] - min_value
# Background correction option 2: subtract the mean value of blank wells
#                                 over the course the experiment
#                                 (Replace B2, D8, G11 with the column
#                                  names of your media-only wells)
#d$blank <- apply(d[, c("B2", "D8", "G11")], 1, mean)
#d$A1 <- d$A1 - d$blank
# Now, call Growthcurver to calculate the metrics using SummarizeGrowth
gc_fit <- SummarizeGrowth(data_t = d_loop[, "time"],
data_n = d_loop[, col_name],
t_trim = trim_at_time,
bg_correct = "none")
# Now, add the metrics from this column to the next row (n) in the
# output data frame, and increment the row counter (n)
d_gc$sample[n] <- col_name
d_gc[n, 2:9] <- c(gc_fit$vals$k,
gc_fit$vals$n0,
gc_fit$vals$r,
gc_fit$vals$t_mid,
gc_fit$vals$t_gen,
gc_fit$vals$auc_l,
gc_fit$vals$auc_e,
gc_fit$vals$sigma)
n <- n + 1
# Finally, plot the raw data and the fitted curve
# Here, I'll just print some of the data points to keep the file size smaller
n_obs <- length(gc_fit$data$t)
idx_to_plot <- 1:20 / 20 * n_obs
plot(gc_fit$data$t[idx_to_plot], gc_fit$data$N[idx_to_plot],
pch = 20,
xlim = c(0, trim_at_time),
ylim = c(0, y_lim_max),
cex = 0.6, xaxt = "n", yaxt = "n")
text(x = trim_at_time / 4, y = y_lim_max, labels = col_name, pos = 1)
if (gc_fit$model=="") {
gc_fit$model<-rep(0,length(tdif[1:grep(trim_at_time, tdif)])-1)
lines(gc_fit$data$t, rep(0,length(tdif[1:grep(trim_at_time, tdif)])-1), col = "indianred3")
}
else #I added 'if...else' for data where no model was fit plots line at y=0
lines(gc_fit$data$t, predict(gc_fit$model), col = "indianred3")
}
}
#dev.off() #to print a pdf remove the hashtag at this line AND at 'pdf(...)' line above
# Load dplyr, ggplot2, and the sample data
library(dplyr)
library(ggplot2)
pca_gc_out <- as_data_frame(gc_out)
# Prepare the gc_out data for the PCA
rownames(pca_gc_out) <- pca_gc_out$sample
# Do the PCA
pca.res <- prcomp(pca_gc_out %>% select(k:sigma), center=TRUE, scale=TRUE)
# Plot the results
as_data_frame(list(PC1=pca.res$x[,1],
PC2=pca.res$x[,2],
samples = rownames(pca.res$x))) %>%
ggplot(aes(x=PC1,y=PC2, label=samples)) +
geom_text(size = 3)
# Do the PCA with percentages in axes
pca.res <- prcomp(pca_gc_out %>% select(k:sigma), center=TRUE, scale=TRUE)
percentage <- round(pca.res$sdev / sum(pca.res$sdev) * 100, 2)
df_out <- as.data.frame(pca.res$x)
percentage <- paste( colnames(df_out), "(", paste( as.character(percentage), "%", ")", sep="") )
# Plot the results
as_data_frame(list(PC1=pca.res$x[,1],
PC2=pca.res$x[,2],
samples = rownames(pca.res$x))) %>%
ggplot(aes(x=PC1,y=PC2, label=samples)) + geom_text(size = 3) + xlab(percentage[1]) + ylab(percentage[2])
par(mfrow = c(8,12))
par(mar = c(0.25,0.25,0.25,0.25))
y<-c(min(gc_out$k), max(gc_out$k))
for (i in 1:12) {
boxplot(gc_out$k[c(i,i+12, i+24)], col="maroon", ylim=y, yaxt="n")
}
y<-c(min(gc_out$r), max(gc_out$r))
for (i in 1:12) {
boxplot(gc_out$r[c(i,i+12, i+24)], col="light coral", ylim=y,yaxt="n")
}
y<-c(min(gc_out$t_mid), max(gc_out$t_mid))
for (i in 1:12) {
boxplot(gc_out$t_mid[c(i,i+12, i+24)], col="goldenrod3", ylim=y,yaxt="n")
}
y<-c(min(gc_out$t_gen), max(gc_out$t_gen))
for (i in 1:12) {
boxplot(gc_out$t_gen[c(i,i+12, i+24)], col="midnightblue", ylim=y,yaxt="n")
}
y<-c(min(gc_out$auc_l), max(gc_out$auc_l))
for (i in 1:12) {
boxplot(gc_out$auc_l[c(i,i+12, i+24)], col="mediumpurple3", ylim=y,yaxt="n")
}
y<-c(min(gc_out$auc_e), max(gc_out$auc_e))
for (i in 1:12) {
boxplot(gc_out$auc_e[c(i,i+12, i+24)], col="tomato3", ylim=y,yaxt="n")
}
par(mfrow=c(1,1),mar = c(3.8,3.8,2,2))
# Plot a histogram of the sigma values in order to check for outliers
hist(gc_out$sigma, breaks= 30,main = "Histogram of sigma values", xlab = "sigma", col = "palegreen3", xlim = c(0, max(gc_out$sigma)))
